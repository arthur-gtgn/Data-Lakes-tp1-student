# Data Lakes & Data Integration

## 1. Build the Repo

### Install the Requirements

Install the necessary packages using `pdm`:

```bash
pdm install
```

### Download the Data

Download the dataset from the following [link](https://www.kaggle.com/api/v1/datasets/download/googleai/pfam-seed-random-split).

**\*Note**: It is possible to download the dataset using the Kaggle API, but this requires you to be logged in, which may make the process longer. To use the Kaggle API, follow these steps:\*

#### A - Ensure you have the Kaggle CLI installed

```bash
pip install kaggle
```

#### B - Authenticate with Kaggle by placing your kaggle.json file (containing your API credentials) in the ~/.kaggle/ directory

#### C - Use the following command to download the dataset

```bash
kaggle datasets download googleai/pfam-seed-random-split
```

### Organize the Data

Move the contents of the dataset (train, dev, test, random_split) to a data/bronze/ folder.

### Unpack the Data

Unpack the data using the unpack_data.py script found in the build folder.

```bash
python build/unpack_data.py --input_dir data/bronze/ --output_file data/bronze/combined_data.csv
```

## 2. Data Analysis

A quick data analysis is at your disposal to help you understand the data in the _data_analysis.ipynb_ notebook. Your goal should be to understand the data, and why the transformations suggested in src/preprocess.py need to be made.

## 3. Data Pre-processing

Data needs to be preprocessed to be stage from a bronze to a silver layer. Your preprocessing script should drop rows with missing values if they exist, encode labels, split data across train/dev/test sets, drop columns and save class weights for training.

```bash
python src/preprocess.py --data_file data/bronze/combined_data.csv --output_dir data/silver/
```
